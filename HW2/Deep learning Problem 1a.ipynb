{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "import sys\n",
    "#You have freedom of using eager execution in tensorflow\n",
    "#Instead of using With tf.Session() as sess you can use sess.run() whenever needed\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "Submitted by: Pakhi Agarwal\n",
    "\n",
    "Problem 1a: Softmax Regression \\& the XOR Problem\n",
    "@author - Alexander G. Ororbia II and Ankur Mali\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeNumGrad(X,y,theta,reg): # returns approximate nabla\n",
    "    # WRITEME: write your code here to complete the routine\n",
    "    eps = 1e-5\n",
    "    \n",
    "    theta_list = [[[1,2], [3,4]], [1,2]]\n",
    "    nabla_n = []\n",
    "    # NOTE: you do not have to use any of the code here in your implementation...\n",
    "        \n",
    "    theta_plus_eps = [[[1,2], [3,4]], [1,2]]\n",
    "    theta_minus_eps = [[[1,2], [3,4]], [1,2]]\n",
    "    ########################################\n",
    "    for i in range(len(theta)):\n",
    "        param = theta[i].eval()\n",
    "        \n",
    "        rep = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        param_grad = np.ndarray(param.shape)\n",
    "        while not rep.finished:\n",
    "        # parameters at (x+eps) and (x-eps)\n",
    "            ix = rep.multi_index\n",
    "           \n",
    "        # Evaluating function at x+eps i.e f(x+eps)\n",
    "            for ix_ in ix:\n",
    "                theta_plus_eps[i][ix_] = param[ix_] + eps\n",
    "                \n",
    "            f_x_plus_eps = computeCost(X,y,theta_plus_eps,reg)\n",
    "        # Reseting theta\n",
    "            for ix_ in ix:\n",
    "                theta_list[i][ix_] = param[ix_] - eps        \n",
    "        # Evaluating function at x i.e f(x-eps)\n",
    "            for ix_ in ix:\n",
    "                theta_minus_eps[i][ix_] = param[ix_] - eps\n",
    "            f_x_minus_eps = computeCost(X,y,theta_minus_eps,reg)\n",
    "        # Reseting theta\n",
    "            for ix_ in ix:\n",
    "                theta_list[i][ix_] = param[ix_] + eps\n",
    "        # gradient at x\n",
    "            \n",
    "            for ix_ in ix:\n",
    "                param_grad[ix_] = (f_x_plus_eps.eval() - f_x_minus_eps.eval())/(2*eps)\n",
    "        # Iterating over all dimensions\n",
    "            rep.iternext()\n",
    "        nabla_n.append(param_grad)  \n",
    "        \n",
    "    ######################################\n",
    "        \n",
    "    return tuple(nabla_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(X, y):\n",
    "# Forward pass\n",
    "    N = int(X.shape[0])\n",
    "    exp_vals = tf.exp(X)\n",
    "    probs = exp_vals / tf.reduce_sum(exp_vals, axis=1, keepdims=True)\n",
    "    loss = -tf.reduce_mean(tf.log(probs))\n",
    "# Backward pass\n",
    "    dX = probs.eval()\n",
    "        \n",
    "    if isinstance(y, np.ndarray):\n",
    "        y0 = y\n",
    "    else:\n",
    "        y0 = y.eval()\n",
    "        \n",
    "    for N_ in range(N):\n",
    "        dX[N_][y0] -= 1\n",
    "    dX /= N\n",
    "    return loss, probs, dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGrad(X,y,theta,reg): # returns nabla\n",
    "    # WRITEME: write your code here to complete the routine\n",
    "    \n",
    "    #######################################\n",
    "    \n",
    "    W, b = theta[0], theta[1]\n",
    "    f = tf.matmul(X,W) + b\n",
    "    _, _, df = softmax_loss(f,y)\n",
    "    dW = tf.matmul(tf.transpose(X), df) + reg * W\n",
    "    db = tf.reduce_sum(df, axis=0)\n",
    "    \n",
    "    ######################################\n",
    "\n",
    "    return (dW,db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X,y,theta,reg):\n",
    "    # WRITEME: write your code here to complete the routine\n",
    "    ########################################\n",
    "    W, b = theta[0], theta[1]\n",
    "    N = X.shape[0]\n",
    "    XX = X.eval()\n",
    "    f = np.matmul(XX, W) + b\n",
    "    data_loss, _, _ = softmax_loss(f,y)\n",
    "    \n",
    "    reg_loss = 0.5 * reg * np.sum(np.dot(W, W))\n",
    "    cost = data_loss + reg_loss\n",
    "    \n",
    "    \n",
    "    #######################################\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,theta):\n",
    "    # WRITEME: write your code here to complete the routine\n",
    "    W, b = theta[0], theta[1]\n",
    "\n",
    "    # evaluate class scores\n",
    "    scores = tf.matmul(X,W) + b\n",
    "    # compute the class probabilities\n",
    "    _, probs, _ = softmax_loss(scores,y)\n",
    "    return scores,probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) #Provide your unique Random seed\n",
    "# Load in the data from disk\n",
    "path = os.getcwd() + '/xor.dat'  \n",
    "data = pd.read_csv(path, header=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]  \n",
    "X = data.iloc[:,0:cols-1]  \n",
    "y = data.iloc[:,cols-1:cols] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from data frames to numpy matrices\n",
    "X = np.array(X.values)  \n",
    "y = np.array(y.values)\n",
    "y = y.flatten()\n",
    "\n",
    "X_tf = tf.constant(X)\n",
    "Y_tf = tf.constant(y)\n",
    "#Train a Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters randomly\n",
    "D = X.shape[1]\n",
    "K = np.amax(y) + 1\n",
    "\n",
    "# initialize parameters in such a way to play nicely with the gradient-check!\n",
    "\n",
    "initializer = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None, dtype=tf.float64)\n",
    "W = tf.Variable(initializer([D, K]), dtype=tf.float64)\n",
    "b = tf.Variable(tf.random_normal([K],dtype=tf.float64),dtype=tf.float64)\n",
    "theta = (W,b)\n",
    "\n",
    "# some hyperparameters\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "initialized = False\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if not initialized:\n",
    "        tf.global_variables_initializer().run()\n",
    "        initialized = True      \n",
    "        \n",
    "    nabla_n = computeNumGrad(X_tf,Y_tf,theta,reg)\n",
    "    nabla = computeGrad(X_tf,Y_tf,theta,reg)\n",
    "    nabla_n = list(nabla_n)\n",
    "    nabla = list(nabla)\n",
    "    #Initialize your variables\n",
    "    for jj in range(0,len(nabla)):\n",
    "        is_incorrect = 0 # set to false\n",
    "        grad = nabla[jj]\n",
    "        grad_n = nabla_n[jj]\n",
    "        grad_sub = tf.subtract(grad_n,grad)\n",
    "        grad_add = tf.add(grad_n,grad)\n",
    "        err = tf.div(tf.norm(grad_sub) , (tf.norm(grad_add)))\n",
    "        \n",
    "        if(err.eval() > 1e-8):\n",
    "            print(\"Param {0} is WRONG, error = {1}\".format(jj, sess.run(err)))\n",
    "        else:\n",
    "            print(\"Param {0} is CORRECT, error = {1}\".format(jj, sess.run(err)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.693401\n",
      "iteration 1: loss 0.693401\n",
      "iteration 2: loss 0.693401\n",
      "iteration 3: loss 0.693400\n",
      "iteration 4: loss 0.693400\n",
      "iteration 5: loss 0.693399\n",
      "iteration 6: loss 0.693399\n",
      "iteration 7: loss 0.693399\n",
      "iteration 8: loss 0.693398\n",
      "iteration 9: loss 0.693398\n",
      "iteration 10: loss 0.693398\n",
      "iteration 11: loss 0.693397\n",
      "iteration 12: loss 0.693397\n",
      "iteration 13: loss 0.693396\n",
      "iteration 14: loss 0.693396\n",
      "iteration 15: loss 0.693396\n",
      "iteration 16: loss 0.693395\n",
      "iteration 17: loss 0.693395\n",
      "iteration 18: loss 0.693395\n",
      "iteration 19: loss 0.693394\n",
      "iteration 20: loss 0.693394\n",
      "iteration 21: loss 0.693393\n",
      "iteration 22: loss 0.693393\n",
      "iteration 23: loss 0.693393\n",
      "iteration 24: loss 0.693392\n",
      "iteration 25: loss 0.693392\n",
      "iteration 26: loss 0.693392\n",
      "iteration 27: loss 0.693391\n",
      "iteration 28: loss 0.693391\n",
      "iteration 29: loss 0.693391\n",
      "iteration 30: loss 0.693390\n",
      "iteration 31: loss 0.693390\n",
      "iteration 32: loss 0.693390\n",
      "iteration 33: loss 0.693389\n",
      "iteration 34: loss 0.693389\n",
      "iteration 35: loss 0.693388\n",
      "iteration 36: loss 0.693388\n",
      "iteration 37: loss 0.693388\n",
      "iteration 38: loss 0.693387\n",
      "iteration 39: loss 0.693387\n",
      "iteration 40: loss 0.693387\n",
      "iteration 41: loss 0.693386\n",
      "iteration 42: loss 0.693386\n",
      "iteration 43: loss 0.693386\n",
      "iteration 44: loss 0.693385\n",
      "iteration 45: loss 0.693385\n",
      "iteration 46: loss 0.693385\n",
      "iteration 47: loss 0.693384\n",
      "iteration 48: loss 0.693384\n",
      "iteration 49: loss 0.693384\n",
      "iteration 50: loss 0.693383\n",
      "iteration 51: loss 0.693383\n",
      "iteration 52: loss 0.693383\n",
      "iteration 53: loss 0.693382\n",
      "iteration 54: loss 0.693382\n",
      "iteration 55: loss 0.693382\n",
      "iteration 56: loss 0.693381\n",
      "iteration 57: loss 0.693381\n",
      "iteration 58: loss 0.693381\n",
      "iteration 59: loss 0.693380\n",
      "iteration 60: loss 0.693380\n",
      "iteration 61: loss 0.693380\n",
      "iteration 62: loss 0.693379\n",
      "iteration 63: loss 0.693379\n",
      "iteration 64: loss 0.693379\n",
      "iteration 65: loss 0.693378\n",
      "iteration 66: loss 0.693378\n",
      "iteration 67: loss 0.693378\n",
      "iteration 68: loss 0.693378\n",
      "iteration 69: loss 0.693377\n",
      "iteration 70: loss 0.693377\n",
      "iteration 71: loss 0.693377\n",
      "iteration 72: loss 0.693376\n",
      "iteration 73: loss 0.693376\n",
      "iteration 74: loss 0.693376\n",
      "iteration 75: loss 0.693375\n",
      "iteration 76: loss 0.693375\n",
      "iteration 77: loss 0.693375\n",
      "iteration 78: loss 0.693374\n",
      "iteration 79: loss 0.693374\n",
      "iteration 80: loss 0.693374\n",
      "iteration 81: loss 0.693374\n",
      "iteration 82: loss 0.693373\n",
      "iteration 83: loss 0.693373\n",
      "iteration 84: loss 0.693373\n",
      "iteration 85: loss 0.693372\n",
      "iteration 86: loss 0.693372\n",
      "iteration 87: loss 0.693372\n",
      "iteration 88: loss 0.693371\n",
      "iteration 89: loss 0.693371\n",
      "iteration 90: loss 0.693371\n",
      "iteration 91: loss 0.693371\n",
      "iteration 92: loss 0.693370\n",
      "iteration 93: loss 0.693370\n",
      "iteration 94: loss 0.693370\n",
      "iteration 95: loss 0.693369\n",
      "iteration 96: loss 0.693369\n",
      "iteration 97: loss 0.693369\n",
      "iteration 98: loss 0.693369\n",
      "iteration 99: loss 0.693368\n",
      "training accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# Re-initialize parameters for generic training\n",
    "initializer = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None, dtype=tf.float64) #You can use Xavier or Ortho for weight init\n",
    "#If using other init compare that with Guassian init and report your findings\n",
    "W = tf.Variable(initializer([D, K]),dtype=tf.float64)\n",
    "b = tf.Variable(tf.random_normal([K],dtype=tf.float64),dtype=tf.float64)\n",
    "theta = (W,b)\n",
    "\n",
    "#play with hyperparameters for better performance \n",
    "n_e = 100 #number of epochs\n",
    "check = 10 # every so many pass/epochs, print loss/error to terminal\n",
    "step_size = 0.001\n",
    "reg = 0.001 # regularization strength\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "with tf.Session() as sess: #You can exclude this ans use sess.run() whenever needed \n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "       \n",
    "    for i in xrange(n_e):\n",
    "        # WRITEME: write your code here to perform a step of gradient descent & record anything else desired for later\n",
    "        \n",
    "        #########################################\n",
    "        \n",
    "        theta = (W.eval(), b.eval())\n",
    "        loss = computeCost(X_tf,Y_tf,theta,reg)\n",
    "        if True:\n",
    "            print(\"iteration %d: loss %f\" % (i, loss.eval()))\n",
    "        #########################################\n",
    "        \n",
    "        # perform a parameter update\n",
    "        # WRITEME: write your update rule(s) here\n",
    "        \n",
    "        ##########################################\n",
    "        dW, db = computeGrad(X_tf,Y_tf,theta,reg)\n",
    "        W = W - step_size*dW\n",
    "        b = b - step_size*db\n",
    "        \n",
    "        #########################################\n",
    " \n",
    "# TODO: remove this line below once you have correctly implemented/gradient-checked your various sub-routines\n",
    "#sys.exit(0) \n",
    "\n",
    "# evaluate training set accuracy\n",
    "    scores, probs = predict(X,theta)\n",
    "    predicted_class = sess.run(tf.argmax(scores, axis=1))\n",
    "    \n",
    "    print ('training accuracy: %.2f%%' % (100*np.mean(predicted_class == y)))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
