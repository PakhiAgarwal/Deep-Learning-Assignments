{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "import sys\n",
    "#You have freedom of using eager execution in tensorflow\n",
    "#Instead of using With tf.Session() as sess you can use sess.run() whenever needed\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(X, y):\n",
    "# Forward pass\n",
    "    N = int(X.shape[0])\n",
    "    X -= tf.reduce_max(X, axis=1, keepdims=True)\n",
    "    exp_vals = tf.exp(X)\n",
    "    probs = exp_vals / tf.reduce_sum(exp_vals, axis=1, keepdims=True)\n",
    "    loss = -tf.reduce_mean(tf.log(probs.eval()[range(N), y]))\n",
    "    #loss = -np.mean(np.log(probs.eval()[range(N), y]))\n",
    "\n",
    "\n",
    "# Backward pass\n",
    "    dX = tf.convert_to_tensor(probs).eval()\n",
    "    dX[range(N), y] -= 1\n",
    "    dX /= N\n",
    "    return loss, probs, dX"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "Problem 2b: 2-Layer MLP for IRIS\n",
    "@author - Alexander G. Ororbia II and ankur mali\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X,y,theta,reg):\n",
    "    # WRITEME: write your code here to complete the routine\n",
    "    ###########################################\n",
    "    W1, b1, W2, b2, W3, b3 = theta[0], theta[1], theta[2], theta[3], theta[4], theta[5]\n",
    "    # fc1\n",
    "    z1 = tf.matmul(X,W1) + b1      \n",
    "    # ReLU1\n",
    "    h1 = tf.maximum(tf.cast(0,tf.float64), tf.cast(z1, tf.float64))   \n",
    "    # fc2\n",
    "    z2 = tf.matmul(h1,W2) + b2     \n",
    "    # ReLU2\n",
    "    h2 = tf.maximum(tf.cast(0,tf.float64), tf.cast(z2,tf.float64))   \n",
    "    # fc3\n",
    "    f = tf.matmul(h2,W3) + b3      \n",
    "    # softmax\n",
    "    data_loss, _, _ = softmax_loss(f, y) \n",
    "    reg_loss = 0.5 * reg * tf.reduce_sum(W1**2) + 0.5 * reg * tf.reduce_sum(W2**2) + 0.5 * reg * tf.reduce_sum(W3**2)\n",
    "    loss = data_loss + reg_loss\n",
    "    \n",
    "    ###########################################\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGrad(X,y,theta,reg): # returns nabla\n",
    "    \n",
    "    #############################################\n",
    "    W1, b1, W2, b2, W3, b3 = theta[0], theta[1], theta[2], theta[3], theta[4], theta[5]\n",
    "    # fc1\n",
    "    z1 = tf.matmul(X,W1) + b1    \n",
    "    # ReLU1  \n",
    "    h1 = tf.maximum(tf.cast(0,tf.float64), tf.cast(z1,tf.float64))   \n",
    "    # fc2\n",
    "    z2 = tf.matmul(h1,W2) + b2     \n",
    "    # ReLU2\n",
    "    h2 = tf.maximum(tf.cast(0,tf.float64), tf.cast(z2,tf.float64))   \n",
    "    # fc3\n",
    "    f = tf.matmul(h2,W3) + b3      \n",
    "    # softmax\n",
    "    _, _, df = softmax_loss(f, y) \n",
    "    \n",
    "    dh2 = tf.matmul(df,tf.transpose(W3))\n",
    "    dz2 = tf.convert_to_tensor(dh2).eval()\n",
    "    dz2[z2.eval() <= 0] = 0\n",
    "    dh1 = tf.matmul(dz2,tf.transpose(W2))\n",
    "    dz1 = tf.convert_to_tensor(dh1).eval()\n",
    "    dz1[z1.eval() <= 0] = 0\n",
    "    \n",
    "    #############################################\n",
    "    # WRITEME: write your code here to complete the routine\n",
    "    \n",
    "    \n",
    "    ###############################################\n",
    "    \n",
    "    dW3 = tf.matmul(tf.transpose(h2), df) + reg * W3\n",
    "    db3 = tf.reduce_sum(df, axis=0)\n",
    "    dW2 = tf.matmul(tf.transpose(h1), dz2) + reg * W2\n",
    "    db2 = tf.reduce_sum(dz2, axis=0)\n",
    "    dW1 = tf.matmul(tf.transpose(X), dz1) + reg * W1\n",
    "    db1 = tf.reduce_sum(dz1, axis=0)\n",
    "    \n",
    "    ################################################\n",
    "    \n",
    "    return (dW1,db1,dW2,db2,dW3,db3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,theta):\n",
    "    # WRITEME: write your code here to complete the routine\n",
    "    \n",
    "    ############################################\n",
    "    W1, b1, W2, b2, W3, b3 = theta[0], theta[1], theta[2], theta[3], theta[4], theta[5]\n",
    "    z1 = tf.matmul(X,W1) + b1        # FC1\n",
    "    h1 = tf.maximum(tf.cast(0,tf.float64),tf.cast(z1,tf.float64))     # ReLU1\n",
    "    z2 = tf.matmul(h1,W2) + b2       # FC2\n",
    "    h2 = tf.maximum(tf.cast(0,tf.float64),tf.cast(z2, tf.float64))     # ReLU2\n",
    "    scores = tf.matmul(h2,W3) + b3   # FC3\n",
    "\n",
    "    probs = tf.exp(scores - np.max(scores))\n",
    "    probs /= tf.reduce_sum(probs)\n",
    "\n",
    "    ###########################################\n",
    "    return scores,probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(X, y, start, end):\n",
    "    # WRITEME: write your code here to complete the routine\n",
    "    \n",
    "    #############################\n",
    "    mb_x = X[start : end, :]\n",
    "    mb_y = y[start : end]\n",
    "    #############################\n",
    "    \n",
    "    return (mb_x, mb_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X,y):\n",
    "    ii = np.arange(X.shape[0])\n",
    "    ii = np.random.shuffle(ii)\n",
    "    X_rand = X[ii]\n",
    "    y_rand = y[ii]\n",
    "    X_rand = X_rand.reshape(X_rand.shape[1:])\n",
    "    y_rand = y_rand.reshape(y_rand.shape[1:])\n",
    "    return (X_rand,y_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# Load in the data from disk\n",
    "path = os.getcwd() + '/iris_train.dat'  \n",
    "data = pd.read_csv(path, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]  \n",
    "X = data.iloc[:,0:cols-1]  \n",
    "y = data.iloc[:,cols-1:cols] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from data frames to numpy matrices\n",
    "X = np.array(X.values)  \n",
    "y = np.array(y.values)\n",
    "y = y.flatten()\n",
    "X_tf = tf.constant(X, dtype=tf.float64)\n",
    "Y_tf = tf.constant(y, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load in validation-set\n",
    "path = os.getcwd() + '/iris_test.dat'\n",
    "data = pd.read_csv(path, header=None) \n",
    "cols = data.shape[1]  \n",
    "X_v = data.iloc[:,0:cols-1]  \n",
    "y_v = data.iloc[:,cols-1:cols] \n",
    "\n",
    "X_v = np.array(X_v.values)  \n",
    "y_v = np.array(y_v.values)\n",
    "y_v = y_v.flatten()\n",
    "\n",
    "X_V_tf = tf.constant(X_v)\n",
    "Y_V_tf = tf.constant(y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters randomly\n",
    "D = X.shape[1]\n",
    "K = np.amax(y) + 1\n",
    "\n",
    "# initialize parameters randomly\n",
    "h = 100 # size of hidden layer\n",
    "h2 = 100 # size of hidden layer\n",
    "initializer = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None, dtype=tf.float64)\n",
    "W = tf.Variable(initializer([D, h]), dtype=tf.float64)\n",
    "b = tf.Variable(tf.random_normal([h], dtype=tf.float64), dtype=tf.float64)\n",
    "W2 = tf.Variable(initializer([h, h2]), dtype=tf.float64)\n",
    "b2 = tf.Variable(tf.random_normal([h2], dtype=tf.float64),dtype=tf.float64)\n",
    "W3 = tf.Variable(initializer([h2, K]),dtype=tf.float64)\n",
    "b3 = tf.Variable(tf.random_normal([K],dtype=tf.float64),dtype=tf.float64)\n",
    "theta = (W,b,W2,b2,W3,b3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some hyperparameters\n",
    "n_e = 100\n",
    "n_b = 10\n",
    "step_size = 0.001 #1e-0\n",
    "#reg = 0 #1e-3 # regularization strength\n",
    "reg = 0.001\n",
    "check =10\n",
    "train_cost = []\n",
    "valid_cost = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "\n",
    "Y_tf = tf.reshape(Y_tf, [num_examples, 1])\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    for i in range(n_e):\n",
    "        #X, y = tf.random_shuffle([X,y]) # re-shuffle the data at epoch start to avoid correlations across mini-batches\n",
    "        # WRITEME: write your code here to perform a step of gradient descent & record anything else desired for later\n",
    "        #          you can use the \"check\" variable to decide when to calculate losses and record/print to screen (as in previous sub-problems)\n",
    "\n",
    "        ####################################\n",
    "        train_loss = computeCost(X,y,theta,reg)\n",
    "        valid_loss = computeCost(X_v,y_v,theta,reg)\n",
    "        train_cost.append(train_loss.eval())\n",
    "        valid_cost.append(valid_loss.eval())\n",
    "        if True:\n",
    "            s = \"iteration %d: training loss = %.2f, validation loss = %.2f\" % (i, train_loss.eval(), valid_loss.eval())\n",
    "            print (s)\n",
    "        \n",
    "        #####################################\n",
    "    \n",
    "        # WRITEME: write the inner training loop here (1 full pass, but via mini-batches instead of using the full batch to estimate the gradient)\n",
    " \n",
    "        s = 0\n",
    "        while (s < num_examples):\n",
    "            # build mini-batch of samples\n",
    "            X_mb, y_mb = create_mini_batch(X,y,s,s + n_b)\n",
    "\n",
    "            # WRITEME: gradient calculations and update rules go here\n",
    "            #########################################\n",
    "            \n",
    "            theta = (W, b, W2, b2, W3, b3)\n",
    "            dW1, db1, dW2, db2, dW3, db3 = computeGrad(X_mb,y_mb,theta,reg)\n",
    "            W = W - step_size * dW1\n",
    "            b = b - step_size * db1\n",
    "            W2 = W2 - step_size * dW2\n",
    "            b2 = b2 - step_size * db2\n",
    "            W3 = W3 - step_size * dW3\n",
    "            b3 = b3 - step_size * db3\n",
    "            \n",
    "            ########################################\n",
    "            s += n_b\n",
    "\n",
    "    print(' > Training loop completed!')\n",
    "# TODO: remove this line below once you have correctly implemented/gradient-checked your various sub-routines\n",
    "#sys.exit(0) \n",
    "\n",
    "    scores, probs = predict(X,theta)\n",
    "    predicted_class = sess.run(tf.argmax(scores, axis=1))\n",
    "    print ('training accuracy: %.2f%%' % (100*np.mean(predicted_class == y)))\n",
    "\n",
    "\n",
    "    scores, probs = predict(X_v,theta)\n",
    "    predicted_class = sess.run(tf.argmax(scores, axis=1))\n",
    "    print ('validation accuracy: %.2f%%' % (100*np.mean(predicted_class == y_v)))\n",
    "\n",
    "    # NOTE: write your plot generation code here (for example, using the \"train_cost\" and \"valid_cost\" list variables)\n",
    "\n",
    "###########################################################\n",
    "\n",
    "plt.plot(range(n_e), train_cost, range(n_e), valid_cost)\n",
    "plt.legend(['training loss', 'validation loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('No. of epochs')\n",
    "plt.savefig(os.getcwd() + '/DLAssign2Problem2b/Loss vs Epoch')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "############################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
